{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289},{"sourceId":12757979,"sourceType":"datasetVersion","datasetId":8065138}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport re\n\n# Load the captions\ndef load_captions(caption_file):\n    captions = {}\n    with open(caption_file, 'r') as f:\n        next(f)  # Skip header\n        for line in f:\n            line = line.strip()\n            if line:\n                # Split only on first comma to handle commas in captions\n                parts = line.split(',', 1)\n                if len(parts) == 2:\n                    image_name = parts[0]\n                    caption = parts[1].strip()\n                    \n                    if image_name not in captions:\n                        captions[image_name] = []\n                    captions[image_name].append(caption)\n    return captions\n\n# Load your captions\ncaptions_dict = load_captions('/kaggle/input/flickr8k/captions.txt')\n\n# Basic exploration\nprint(f\"Total images: {len(captions_dict)}\")\nprint(f\"Total captions: {sum(len(caps) for caps in captions_dict.values())}\")\n\n# Show some examples\nsample_images = list(captions_dict.keys())[:3]\nfor img in sample_images:\n    print(f\"\\n{img}:\")\n    for i, cap in enumerate(captions_dict[img]):\n        print(f\"  {i+1}. {cap}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:27:49.694045Z","iopub.execute_input":"2025-08-13T21:27:49.694694Z","iopub.status.idle":"2025-08-13T21:27:50.026223Z","shell.execute_reply.started":"2025-08-13T21:27:49.694659Z","shell.execute_reply":"2025-08-13T21:27:50.025616Z"}},"outputs":[{"name":"stdout","text":"Total images: 8091\nTotal captions: 40455\n\n1000268201_693b08cb0e.jpg:\n  1. A child in a pink dress is climbing up a set of stairs in an entry way .\n  2. A girl going into a wooden building .\n  3. A little girl climbing into a wooden playhouse .\n  4. A little girl climbing the stairs to her playhouse .\n  5. A little girl in a pink dress going into a wooden cabin .\n\n1001773457_577c3a7d70.jpg:\n  1. A black dog and a spotted dog are fighting\n  2. A black dog and a tri-colored dog playing with each other on the road .\n  3. A black dog and a white dog with brown spots are staring at each other in the street .\n  4. Two dogs of different breeds looking at each other on the road .\n  5. Two dogs on pavement moving toward each other .\n\n1002674143_1b742ab4b8.jpg:\n  1. A little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .\n  2. A little girl is sitting in front of a large painted rainbow .\n  3. A small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it .\n  4. There is a girl with pigtails sitting in front of a rainbow painting .\n  5. Young girl with pigtails painting outside in the grass .\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import string\n\ndef preprocess_caption(caption):\n    \"\"\"Clean and preprocess a single caption\"\"\"\n    # Convert to lowercase\n    caption = caption.lower()\n    \n    # Remove extra whitespace\n    caption = caption.strip()\n    \n    # Remove punctuation except periods (we'll handle them specially)\n    caption = re.sub(r'[^\\w\\s.]', '', caption)\n    \n    # Remove multiple spaces\n    caption = re.sub(r'\\s+', ' ', caption)\n    \n    # Remove trailing period if present\n    caption = caption.rstrip('.')\n    \n    # Add start and end tokens\n    caption = '<start> ' + caption + ' <end>'\n    \n    return caption\n\n# Preprocess all captions\nprocessed_captions = {}\nall_words = []\n\nfor image_name, caption_list in captions_dict.items():\n    processed_captions[image_name] = []\n    for caption in caption_list:\n        processed_cap = preprocess_caption(caption)\n        processed_captions[image_name].append(processed_cap)\n        # Collect all words for vocabulary\n        all_words.extend(processed_cap.split())\n\nprint(\"Sample processed captions:\")\nsample_img = list(processed_captions.keys())[0]\nfor i, cap in enumerate(processed_captions[sample_img][:3]):\n    print(f\"{i+1}. {cap}\")\n\n# Build vocabulary\nword_counts = Counter(all_words)\nprint(f\"\\nTotal unique words: {len(word_counts)}\")\nprint(f\"Most common words: {word_counts.most_common(10)}\")\n\n# Create vocabulary (words appearing at least 5 times)\nmin_word_count = 5\nvocab = [word for word, count in word_counts.items() if count >= min_word_count]\n\n# Add special tokens if not present\nspecial_tokens = ['<start>', '<end>', '<unk>', '<pad>']\nfor token in special_tokens:\n    if token not in vocab:\n        vocab.append(token)\n\nprint(f\"\\nVocabulary size (min_count={min_word_count}): {len(vocab)}\")\nprint(f\"Special tokens added: {special_tokens}\")\n\n# Create word-to-index and index-to-word mappings\nword_to_idx = {word: idx for idx, word in enumerate(vocab)}\nidx_to_word = {idx: word for word, idx in word_to_idx.items()}\n\nprint(f\"\\nSample word mappings:\")\nfor word in ['<start>', 'a', 'dog', 'running', '<end>']:\n    if word in word_to_idx:\n        print(f\"'{word}' -> {word_to_idx[word]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:27:55.999348Z","iopub.execute_input":"2025-08-13T21:27:56.000142Z","iopub.status.idle":"2025-08-13T21:27:56.388258Z","shell.execute_reply.started":"2025-08-13T21:27:56.000117Z","shell.execute_reply":"2025-08-13T21:27:56.387477Z"}},"outputs":[{"name":"stdout","text":"Sample processed captions:\n1. <start> a child in a pink dress is climbing up a set of stairs in an entry way  <end>\n2. <start> a girl going into a wooden building  <end>\n3. <start> a little girl climbing into a wooden playhouse  <end>\n\nTotal unique words: 8836\nMost common words: [('a', 62986), ('<start>', 40455), ('<end>', 40455), ('in', 18974), ('the', 18418), ('on', 10743), ('is', 9345), ('and', 8851), ('dog', 8136), ('with', 7765)]\n\nVocabulary size (min_count=5): 2996\nSpecial tokens added: ['<start>', '<end>', '<unk>', '<pad>']\n\nSample word mappings:\n'<start>' -> 0\n'a' -> 1\n'dog' -> 26\n'running' -> 110\n'<end>' -> 14\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\n\nclass Flickr8kDataset(Dataset):\n    def __init__(self, image_dir, captions_dict, word_to_idx, transform=None):\n        self.image_dir = image_dir\n        self.word_to_idx = word_to_idx\n        self.transform = transform\n        \n        # Create list of (image_path, caption) pairs\n        self.data = []\n        for image_name, caption_list in captions_dict.items():\n            for caption in caption_list:\n                self.data.append((image_name, caption))\n        \n        print(f\"Dataset created with {len(self.data)} image-caption pairs\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        image_name, caption = self.data[idx]\n        \n        # Load and transform image\n        image_path = os.path.join(self.image_dir, image_name)\n        image = Image.open(image_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        # Convert caption to tensor\n        caption_tokens = caption.split()\n        caption_indices = []\n        \n        for token in caption_tokens:\n            if token in self.word_to_idx:\n                caption_indices.append(self.word_to_idx[token])\n            else:\n                caption_indices.append(self.word_to_idx['<unk>'])\n        \n        caption_tensor = torch.tensor(caption_indices, dtype=torch.long)\n        \n        return image, caption_tensor\n\n# Define image transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])  # ImageNet normalization\n])\n\n# Create dataset\ndataset = Flickr8kDataset(\n    image_dir='/kaggle/input/flickr8k/Images',  # adjust path\n    captions_dict=processed_captions,\n    word_to_idx=word_to_idx,\n    transform=transform\n)\n\n# Test the dataset\nprint(\"Testing dataset...\")\nsample_image, sample_caption = dataset[0]\nprint(f\"Image shape: {sample_image.shape}\")\nprint(f\"Caption tensor: {sample_caption}\")\nprint(f\"Caption length: {len(sample_caption)}\")\n\n# Convert back to words to verify\nsample_words = [idx_to_word[idx.item()] for idx in sample_caption]\nprint(f\"Caption as words: {' '.join(sample_words)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:28:00.433688Z","iopub.execute_input":"2025-08-13T21:28:00.434223Z","iopub.status.idle":"2025-08-13T21:28:07.675707Z","shell.execute_reply.started":"2025-08-13T21:28:00.434204Z","shell.execute_reply":"2025-08-13T21:28:07.675015Z"}},"outputs":[{"name":"stdout","text":"Dataset created with 40455 image-caption pairs\nTesting dataset...\nImage shape: torch.Size([3, 224, 224])\nCaption tensor: tensor([   0,    1,    2,    3,    1,    4,    5,    6,    7,    8,    1,    9,\n          10,   11,    3,   12, 2994,   13,   14])\nCaption length: 19\nCaption as words: <start> a child in a pink dress is climbing up a set of stairs in an <unk> way <end>\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def collate_fn(batch):\n    \"\"\"Custom collate function to handle variable length captions\"\"\"\n    images, captions = zip(*batch)\n    \n    # Stack images\n    images = torch.stack(images)\n    \n    # Get caption lengths\n    lengths = [len(cap) for cap in captions]\n    max_length = max(lengths)\n    \n    # Pad captions to max length\n    padded_captions = torch.zeros(len(captions), max_length, dtype=torch.long)\n    \n    for i, caption in enumerate(captions):\n        padded_captions[i, :len(caption)] = caption\n        # Pad with <pad> token\n        if len(caption) < max_length:\n            padded_captions[i, len(caption):] = word_to_idx['<pad>']\n    \n    return images, padded_captions, torch.tensor(lengths)\n\n# Test collate function\ntest_loader = DataLoader(dataset, batch_size=3, shuffle=False, collate_fn=collate_fn)\ntest_batch = next(iter(test_loader))\ntest_images, test_captions, test_lengths = test_batch\n\nprint(f\"\\nBatch shapes:\")\nprint(f\"Images: {test_images.shape}\")\nprint(f\"Captions: {test_captions.shape}\")\nprint(f\"Lengths: {test_lengths}\")\n\nprint(f\"\\nFirst caption in batch:\")\nfirst_cap_words = [idx_to_word[idx.item()] for idx in test_captions[0]]\nprint(' '.join(first_cap_words))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:28:09.920797Z","iopub.execute_input":"2025-08-13T21:28:09.921176Z","iopub.status.idle":"2025-08-13T21:28:09.966528Z","shell.execute_reply.started":"2025-08-13T21:28:09.921153Z","shell.execute_reply":"2025-08-13T21:28:09.965715Z"}},"outputs":[{"name":"stdout","text":"\nBatch shapes:\nImages: torch.Size([3, 3, 224, 224])\nCaptions: torch.Size([3, 19])\nLengths: tensor([19,  9, 10])\n\nFirst caption in batch:\n<start> a child in a pink dress is climbing up a set of stairs in an <unk> way <end>\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport torch\nfrom torch.utils.data import Subset\n\n# Split image names (not individual captions) to avoid data leakage\nimage_names = list(processed_captions.keys())\nprint(f\"Total images: {len(image_names)}\")\n\n# 80% train, 20% validation split\ntrain_images, val_images = train_test_split(\n    image_names, \n    test_size=0.2, \n    random_state=97,\n    shuffle=True\n)\n\nprint(f\"Train images: {len(train_images)}\")\nprint(f\"Validation images: {len(val_images)}\")\n\n# Create separate caption dictionaries for train and val\ntrain_captions = {img: processed_captions[img] for img in train_images}\nval_captions = {img: processed_captions[img] for img in val_images}\n\n# Verify split\ntrain_caption_count = sum(len(caps) for caps in train_captions.values())\nval_caption_count = sum(len(caps) for caps in val_captions.values())\n\nprint(f\"Train captions: {train_caption_count}\")\nprint(f\"Val captions: {val_caption_count}\")\nprint(f\"Total: {train_caption_count + val_caption_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:28:12.842484Z","iopub.execute_input":"2025-08-13T21:28:12.843069Z","iopub.status.idle":"2025-08-13T21:28:13.408091Z","shell.execute_reply.started":"2025-08-13T21:28:12.843044Z","shell.execute_reply":"2025-08-13T21:28:13.407414Z"}},"outputs":[{"name":"stdout","text":"Total images: 8091\nTrain images: 6472\nValidation images: 1619\nTrain captions: 32360\nVal captions: 8095\nTotal: 40455\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Create train and validation datasets\ntrain_dataset = Flickr8kDataset(\n    image_dir='/kaggle/input/flickr8k/Images',\n    captions_dict=train_captions,\n    word_to_idx=word_to_idx,\n    transform=transform\n)\n\nval_dataset = Flickr8kDataset(\n    image_dir='/kaggle/input/flickr8k/Images',\n    captions_dict=val_captions,\n    word_to_idx=word_to_idx,\n    transform=transform\n)\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Val dataset size: {len(val_dataset)}\")\n\n# Create data loaders\nbatch_size = 16  # Power of 2, adjust based on GPU memory\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=2,  # For faster data loading\n    pin_memory=True  # For GPU efficiency\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=batch_size,\n    shuffle=False,  # No need to shuffle validation\n    collate_fn=collate_fn,\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"Train batches: {len(train_loader)}\")\nprint(f\"Val batches: {len(val_loader)}\")\n\n# Test the loaders\nprint(\"\\nTesting train loader...\")\ntrain_batch = next(iter(train_loader))\ntrain_images, train_captions, train_lengths = train_batch\n\nprint(f\"Train batch - Images: {train_images.shape}\")\nprint(f\"Train batch - Captions: {train_captions.shape}\")\nprint(f\"Train batch - Lengths: {train_lengths}\")\n\nprint(f\"Max caption length in batch: {train_captions.shape[1]}\")\nprint(f\"Caption lengths: {train_lengths.tolist()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:28:15.704112Z","iopub.execute_input":"2025-08-13T21:28:15.704501Z","iopub.status.idle":"2025-08-13T21:28:16.601454Z","shell.execute_reply.started":"2025-08-13T21:28:15.704480Z","shell.execute_reply":"2025-08-13T21:28:16.600742Z"}},"outputs":[{"name":"stdout","text":"Dataset created with 32360 image-caption pairs\nDataset created with 8095 image-caption pairs\nTrain dataset size: 32360\nVal dataset size: 8095\nTrain batches: 2023\nVal batches: 506\n\nTesting train loader...\nTrain batch - Images: torch.Size([16, 3, 224, 224])\nTrain batch - Captions: torch.Size([16, 25])\nTrain batch - Lengths: tensor([15, 13, 13, 13,  9, 15, 25,  9, 14, 14, 12, 14, 12, 13, 10, 13])\nMax caption length in batch: 25\nCaption lengths: [15, 13, 13, 13, 9, 15, 25, 9, 14, 14, 12, 14, 12, 13, 10, 13]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nimport math\n\nclass ImageEncoder(nn.Module):\n    \"\"\"EfficientNet-B0 encoder (frozen)\"\"\"\n    def __init__(self, embed_dim=256):\n        super().__init__()\n        # Load pretrained EfficientNet-B0\n        self.efficientnet = models.efficientnet_b0(pretrained=True)\n        \n        # Freeze all parameters\n        for param in self.efficientnet.parameters():\n            param.requires_grad = False\n        \n        # Get the feature dimension before the classifier\n        # EfficientNet-B0 has 1280 features before classifier\n        self.feature_dim = self.efficientnet.classifier[1].in_features\n        \n        # Remove the classifier\n        self.efficientnet.classifier = nn.Identity()\n        \n        # Add projection layer to embed_dim\n        self.projection = nn.Linear(self.feature_dim, embed_dim)\n        \n        print(f\"ImageEncoder: EfficientNet-B0 -> {embed_dim} dims\")\n        print(f\"Frozen parameters: {sum(1 for p in self.efficientnet.parameters())}\")\n    \n    def forward(self, images):\n        # Extract features\n        features = self.efficientnet(images)  # [batch, 1280]\n        \n        # Project to embedding dimension\n        embedded = self.projection(features)  # [batch, embed_dim]\n        \n        return embedded\n\n# Test the encoder\nprint(\"Testing ImageEncoder...\")\nencoder = ImageEncoder(embed_dim=256)\n\n# Test with a batch from train_loader\ntest_images, _, _ = next(iter(train_loader))\nprint(f\"Input images shape: {test_images.shape}\")\n\nwith torch.no_grad():\n    encoded_features = encoder(test_images)\n    print(f\"Encoded features shape: {encoded_features.shape}\")\n\n# Check trainable parameters\ntrainable_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in encoder.parameters())\nprint(f\"Trainable params: {trainable_params:,}\")\nprint(f\"Total params: {total_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:28:19.186874Z","iopub.execute_input":"2025-08-13T21:28:19.187146Z","iopub.status.idle":"2025-08-13T21:28:20.786197Z","shell.execute_reply.started":"2025-08-13T21:28:19.187122Z","shell.execute_reply":"2025-08-13T21:28:20.785253Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n","output_type":"stream"},{"name":"stdout","text":"Testing ImageEncoder...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20.5M/20.5M [00:00<00:00, 161MB/s]","output_type":"stream"},{"name":"stdout","text":"ImageEncoder: EfficientNet-B0 -> 256 dims\nFrozen parameters: 211\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Input images shape: torch.Size([16, 3, 224, 224])\nEncoded features shape: torch.Size([16, 256])\nTrainable params: 327,936\nTotal params: 4,335,484\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    \"\"\"Positional encoding for transformer\"\"\"\n    def __init__(self, d_model, max_len=100):\n        super().__init__()\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        \n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           -(math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n    \n    def forward(self, x):\n        # x shape: [batch, seq_len, d_model]\n        return x + self.pe[:, :x.size(1)]\n\n# Test positional encoding\npos_encoding = PositionalEncoding(d_model=256, max_len=50)\ntest_input = torch.randn(2, 10, 256)  # [batch, seq_len, embed_dim]\npos_output = pos_encoding(test_input)\nprint(f\"Positional encoding test: {test_input.shape} -> {pos_output.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:28:22.084185Z","iopub.execute_input":"2025-08-13T21:28:22.084525Z","iopub.status.idle":"2025-08-13T21:28:22.102061Z","shell.execute_reply.started":"2025-08-13T21:28:22.084495Z","shell.execute_reply":"2025-08-13T21:28:22.101362Z"}},"outputs":[{"name":"stdout","text":"Positional encoding test: torch.Size([2, 10, 256]) -> torch.Size([2, 10, 256])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class TransformerDecoder(nn.Module):\n    \"\"\"2-layer Transformer decoder for caption generation\"\"\"\n    def __init__(self, vocab_size, embed_dim=256, num_heads=4, num_layers=2, \n                 max_seq_len=50, dropout=0.1):\n        super().__init__()\n        \n        self.embed_dim = embed_dim\n        self.vocab_size = vocab_size\n        self.max_seq_len = max_seq_len\n        \n        # Word embedding layer\n        self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n        \n        # Positional encoding\n        self.pos_encoding = PositionalEncoding(embed_dim, max_seq_len)\n        \n        # Transformer decoder layers\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=embed_dim,\n            nhead=num_heads,\n            dim_feedforward=embed_dim * 4,  # Standard is 4x\n            dropout=dropout,\n            batch_first=True  # Important: batch dimension first\n        )\n        \n        self.transformer_decoder = nn.TransformerDecoder(\n            decoder_layer, \n            num_layers=num_layers\n        )\n        \n        # Output projection to vocabulary\n        self.output_projection = nn.Linear(embed_dim, vocab_size)\n        \n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n        \n        print(f\"TransformerDecoder: {num_layers} layers, {num_heads} heads\")\n        print(f\"Vocabulary size: {vocab_size}\")\n    \n    def create_padding_mask(self, sequences, lengths):\n        \"\"\"Create mask for padded tokens\"\"\"\n        batch_size, max_len = sequences.shape\n        device = sequences.device  # Get device from input tensor\n        \n        mask = torch.arange(max_len, device=device).expand(batch_size, max_len) >= lengths.unsqueeze(1)\n        return mask\n    \n    def create_causal_mask(self, seq_len):\n        \"\"\"Create causal (look-ahead) mask for decoder\"\"\"\n        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n        return mask.bool()\n    \n    def forward(self, image_features, captions, caption_lengths, training=True):\n        batch_size, seq_len = captions.shape\n        \n        # Word embeddings\n        word_embeds = self.word_embedding(captions)  # [batch, seq_len, embed_dim]\n        \n        # Add positional encoding\n        word_embeds = self.pos_encoding(word_embeds)\n        word_embeds = self.dropout(word_embeds)\n        \n        # Prepare image features as memory (encoder output)\n        # Transformer expects [batch, src_seq_len, embed_dim]\n        # Our image features are [batch, embed_dim], so we add sequence dimension\n        memory = image_features.unsqueeze(1)  # [batch, 1, embed_dim]\n        \n        # Create masks\n        if training:\n            # Causal mask prevents looking ahead\n            causal_mask = self.create_causal_mask(seq_len).to(captions.device)\n        else:\n            causal_mask = None\n        \n        # Padding mask for target sequence\n        tgt_key_padding_mask = self.create_padding_mask(captions, caption_lengths).to(captions.device)\n        \n        # Transformer decoder\n        decoder_output = self.transformer_decoder(\n            tgt=word_embeds,\n            memory=memory,\n            tgt_mask=causal_mask,\n            tgt_key_padding_mask=tgt_key_padding_mask\n        )\n        \n        # Project to vocabulary\n        logits = self.output_projection(decoder_output)  # [batch, seq_len, vocab_size]\n        \n        return logits\n\n# Test the decoder\nprint(\"Testing TransformerDecoder...\")\ndecoder = TransformerDecoder(\n    vocab_size=len(vocab),\n    embed_dim=256,\n    num_heads=4,\n    num_layers=2\n)\n\n# Test with encoder output and captions\ntest_images, test_captions, test_lengths = next(iter(train_loader))\ntest_encoded = encoder(test_images)\n\nprint(f\"Image features shape: {test_encoded.shape}\")\nprint(f\"Input captions shape: {test_captions.shape}\")\nprint(f\"Caption lengths: {test_lengths[:5]}\")  # Show first 5\n\n# Forward pass\nwith torch.no_grad():\n    logits = decoder(test_encoded, test_captions, test_lengths, training=True)\n    print(f\"Output logits shape: {logits.shape}\")\n    print(f\"Expected: [batch_size, seq_len, vocab_size] = [{test_captions.shape[0]}, {test_captions.shape[1]}, {len(vocab)}]\")\n\n# Check decoder parameters\ndecoder_trainable = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\nprint(f\"Decoder trainable params: {decoder_trainable:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:28:24.084226Z","iopub.execute_input":"2025-08-13T21:28:24.084511Z","iopub.status.idle":"2025-08-13T21:28:25.438276Z","shell.execute_reply.started":"2025-08-13T21:28:24.084490Z","shell.execute_reply":"2025-08-13T21:28:25.437338Z"}},"outputs":[{"name":"stdout","text":"Testing TransformerDecoder...\nTransformerDecoder: 2 layers, 4 heads\nVocabulary size: 2996\nImage features shape: torch.Size([16, 256])\nInput captions shape: torch.Size([16, 19])\nCaption lengths: tensor([12, 19, 10, 10, 10])\nOutput logits shape: torch.Size([16, 19, 2996])\nExpected: [batch_size, seq_len, vocab_size] = [16, 19, 2996]\nDecoder trainable params: 3,643,828\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class ImageCaptionGenerator(nn.Module):\n    \"\"\"Complete Image Caption Generator: EfficientNet-B0 + Transformer Decoder\"\"\"\n    def __init__(self, vocab_size, embed_dim=256, num_heads=4, num_layers=2, \n                 max_seq_len=50, dropout=0.1):\n        super().__init__()\n        \n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        self.max_seq_len = max_seq_len\n        \n        # Image encoder\n        self.encoder = ImageEncoder(embed_dim=embed_dim)\n        \n        # Text decoder\n        self.decoder = TransformerDecoder(\n            vocab_size=vocab_size,\n            embed_dim=embed_dim,\n            num_heads=num_heads,\n            num_layers=num_layers,\n            max_seq_len=max_seq_len,\n            dropout=dropout\n        )\n        \n        # Store special token indices\n        self.start_token = word_to_idx['<start>']\n        self.end_token = word_to_idx['<end>']\n        self.pad_token = word_to_idx['<pad>']\n        \n        print(f\"Complete model created!\")\n        self.print_model_info()\n    \n    def generate_beam_search(self, images, beam_size=3, max_length=20):\n        \"\"\"Generate captions using beam search for better diversity\"\"\"\n        self.eval()\n        \n        # Encode images\n        image_features = self.encoder(images)\n        batch_size = image_features.shape[0]\n        device = images.device\n        \n        # Process each image separately (beam search is typically done per image)\n        all_results = []\n        \n        for batch_idx in range(batch_size):\n            single_image_features = image_features[batch_idx:batch_idx+1]  # [1, embed_dim]\n            \n            # Initialize beam with start token\n            # Each beam item: (sequence, cumulative_log_prob)\n            beam = [([self.start_token], 0.0)]\n            \n            for step in range(max_length):\n                candidates = []\n                \n                for sequence, score in beam:\n                    # Skip if sequence already ended\n                    if sequence[-1] == self.end_token:\n                        candidates.append((sequence, score))\n                        continue\n                    \n                    # Convert sequence to tensor\n                    seq_tensor = torch.tensor([sequence], dtype=torch.long, device=device)\n                    length_tensor = torch.tensor([len(sequence)], device=device)\n                    \n                    # Get next token probabilities\n                    with torch.no_grad():\n                        logits = self.decoder(single_image_features, seq_tensor, length_tensor, training=False)\n                        next_token_logits = logits[0, -1, :]  # [vocab_size]\n                        log_probs = F.log_softmax(next_token_logits, dim=0)\n                    \n                    # Get top beam_size candidates\n                    top_log_probs, top_indices = torch.topk(log_probs, beam_size)\n                    \n                    # Add candidates\n                    for log_prob, token_idx in zip(top_log_probs, top_indices):\n                        new_sequence = sequence + [token_idx.item()]\n                        new_score = score + log_prob.item()\n                        candidates.append((new_sequence, new_score))\n                \n                # Keep top beam_size candidates\n                candidates.sort(key=lambda x: x[1], reverse=True)  # Sort by score (higher better)\n                beam = candidates[:beam_size]\n                \n                # Stop if all beams have ended\n                if all(seq[-1] == self.end_token for seq, _ in beam):\n                    break\n            \n            # Get best sequence for this image\n            best_sequence = beam[0][0]  # Highest scoring sequence\n            best_tensor = torch.tensor(best_sequence, dtype=torch.long, device=device)\n            all_results.append(best_tensor)\n        \n        # Pad sequences to same length for batch return\n        max_len = max(len(seq) for seq in all_results)\n        \n        batched_results = torch.full((batch_size, max_len), self.pad_token, \n                                    dtype=torch.long, device=device)\n        \n        for i, seq in enumerate(all_results):\n            batched_results[i, :len(seq)] = seq\n        \n        return batched_results\n    \n    def print_model_info(self):\n        \"\"\"Print model parameter information\"\"\"\n        total_params = sum(p.numel() for p in self.parameters())\n        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n        \n        print(f\"Total parameters: {total_params:,}\")\n        print(f\"Trainable parameters: {trainable_params:,}\")\n        print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n        \n        # Check if under 4M limit\n        if trainable_params < 4_000_000:\n            print(f\"✅ Under 4M parameter limit!\")\n        else:\n            print(f\"❌ Exceeds 4M parameter limit by {trainable_params - 4_000_000:,}\")\n    \n    def forward(self, images, captions=None, caption_lengths=None):\n        \"\"\"Forward pass for training\"\"\"\n        # Encode images\n        image_features = self.encoder(images)\n        \n        if captions is not None:\n            # Training mode: use teacher forcing\n            logits = self.decoder(image_features, captions, caption_lengths, training=True)\n            return logits\n        else:\n            # Inference mode: generate captions\n            return self.generate_caption(image_features)\n    \n    def generate_caption(self, image_features, max_length=20, temperature=1.0):\n        \"\"\"Generate caption using greedy decoding\"\"\"\n        batch_size = image_features.shape[0]\n        device = image_features.device\n        \n        # Start with <start> token\n        generated = torch.full((batch_size, 1), self.start_token, \n                              dtype=torch.long, device=device)\n        \n        for _ in range(max_length):\n            # Get current sequence length\n            seq_len = generated.shape[1]\n            lengths = torch.full((batch_size,), seq_len, device=device)\n            \n            # Forward pass\n            with torch.no_grad():\n                logits = self.decoder(image_features, generated, lengths, training=False)\n                \n                # Get next token probabilities (last position)\n                next_token_logits = logits[:, -1, :] / temperature\n                next_token_probs = F.softmax(next_token_logits, dim=-1)\n                \n                # Greedy sampling (take most probable token)\n                next_token = torch.argmax(next_token_probs, dim=-1).unsqueeze(1)\n            \n            # Append to generated sequence\n            generated = torch.cat([generated, next_token], dim=1)\n            \n            # Check if all sequences have generated <end> token\n            if torch.all(next_token.squeeze() == self.end_token):\n                break\n        \n        return generated\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:28:55.392502Z","iopub.execute_input":"2025-08-13T21:28:55.393220Z","iopub.status.idle":"2025-08-13T21:28:55.410244Z","shell.execute_reply.started":"2025-08-13T21:28:55.393187Z","shell.execute_reply":"2025-08-13T21:28:55.409456Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Create the complete model\nmodel = ImageCaptionGenerator(\n    vocab_size=len(vocab),\n    embed_dim=256,\n    num_heads=4,\n    num_layers=2,\n    max_seq_len=50,\n    dropout=0.1\n)\n\n\n## Load best model and evaluate\nprint(\"Loading best model for evaluation...\")\ncheckpoint = torch.load('/kaggle/input/imagecaption-after8/model_checkpoints_after_8/best_model.pth')\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\n# Test the complete model\nprint(\"\\nTesting complete model...\")\ntest_images, test_captions, test_lengths = next(iter(train_loader))\n\n# Training forward pass\nprint(\"Training mode:\")\nwith torch.no_grad():\n    training_logits = model(test_images, test_captions, test_lengths)\n    print(f\"Training output shape: {training_logits.shape}\")\n\n# Inference mode\nprint(\"\\nInference mode:\")\nwith torch.no_grad():\n    generated_captions = model(test_images[:2])  # Generate for first 2 images\n    print(f\"Generated captions shape: {generated_captions.shape}\")\n    \n    # Convert to words\n    for i in range(2):\n        caption_indices = generated_captions[i].cpu().tolist()\n        caption_words = [idx_to_word[idx] for idx in caption_indices]\n        print(f\"Generated caption {i+1}: {' '.join(caption_words)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:29:00.217748Z","iopub.execute_input":"2025-08-13T21:29:00.218030Z","iopub.status.idle":"2025-08-13T21:29:03.553261Z","shell.execute_reply.started":"2025-08-13T21:29:00.218006Z","shell.execute_reply":"2025-08-13T21:29:03.552373Z"}},"outputs":[{"name":"stdout","text":"ImageEncoder: EfficientNet-B0 -> 256 dims\nFrozen parameters: 211\nTransformerDecoder: 2 layers, 4 heads\nVocabulary size: 2996\nComplete model created!\nTotal parameters: 7,979,312\nTrainable parameters: 3,971,764\nFrozen parameters: 4,007,548\n✅ Under 4M parameter limit!\nLoading best model for evaluation...\n\nTesting complete model...\nTraining mode:\nTraining output shape: torch.Size([16, 20, 2996])\n\nInference mode:\nGenerated captions shape: torch.Size([2, 15])\nGenerated caption 1: <start> a dog stands in the water <end> <end> <end> <end> <end> <end> <end> <end>\nGenerated caption 2: <start> a man in a blue jacket and a backpack and a <unk> <unk> <end>\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport json\nfrom datetime import datetime\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nclass TrainingManager:\n    def __init__(self, model, train_loader, val_loader, word_to_idx, idx_to_word, \n                 checkpoint_dir='/kaggle/working/model_checkpoints', device='cuda'):\n        self.model = model\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.word_to_idx = word_to_idx\n        self.idx_to_word = idx_to_word\n        self.device = device\n        self.checkpoint_dir = checkpoint_dir\n        \n        # Create checkpoint directory\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        \n        # Training components\n        self.criterion = nn.CrossEntropyLoss(ignore_index=word_to_idx['<pad>'])\n        \n        # Only train decoder + projection parameters\n        trainable_params = [p for p in model.parameters() if p.requires_grad]\n        self.optimizer = optim.Adam(trainable_params, lr=1e-3, weight_decay=1e-4)\n        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', patience=2, \n                                          factor=0.5)\n        \n        # Training state\n        self.start_epoch = 0\n        self.best_val_loss = float('inf')\n        self.train_losses = []\n        self.val_losses = []\n        \n        print(f\"Training setup complete!\")\n        print(f\"Device: {device}\")\n        print(f\"Trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n    \n    def save_checkpoint(self, epoch, train_loss, val_loss, is_best=False):\n        \"\"\"Save training checkpoint\"\"\"\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'train_losses': self.train_losses,\n            'val_losses': self.val_losses,\n            'best_val_loss': self.best_val_loss,\n            'train_loss': train_loss,\n            'val_loss': val_loss,\n            'word_to_idx': self.word_to_idx,\n            'idx_to_word': self.idx_to_word,\n            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        }\n        \n        # Save last checkpoint\n        last_path = os.path.join(self.checkpoint_dir, 'last_checkpoint.pth')\n        torch.save(checkpoint, last_path)\n        print(f\"Saved checkpoint: {last_path}\")\n        \n        # Save best model\n        if is_best:\n            best_path = os.path.join(self.checkpoint_dir, 'best_model.pth')\n            torch.save(checkpoint, best_path)\n            print(f\"🎉 New best model saved: {best_path} (val_loss: {val_loss:.4f})\")\n    \n    def load_checkpoint(self, checkpoint_path=r\"/kaggle/input/model\"):\n        \"\"\"Load checkpoint to resume training\"\"\"\n        if os.path.exists(checkpoint_path):\n            print(f\"Loading checkpoint: {checkpoint_path}\")\n            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n            \n            self.model.load_state_dict(checkpoint['model_state_dict'])\n            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n            \n            self.start_epoch = checkpoint['epoch'] + 1\n            self.train_losses = checkpoint['train_losses']\n            self.val_losses = checkpoint['val_losses']\n            self.best_val_loss = checkpoint['best_val_loss']\n            \n            print(f\"Resumed from epoch {self.start_epoch}\")\n            print(f\"Best validation loss so far: {self.best_val_loss:.4f}\")\n            return True\n        else:\n            print(f\"No checkpoint found at {checkpoint_path}\")\n            return False\n    \n    def train_epoch(self):\n        \"\"\"Train for one epoch\"\"\"\n        self.model.train()\n        total_loss = 0\n        num_batches = len(self.train_loader)\n        \n        for batch_idx, (images, captions, lengths) in enumerate(tqdm.tqdm(self.train_loader)):\n            images = images.to(self.device)\n            captions = captions.to(self.device)\n            lengths = lengths.to(self.device)\n            \n            # Forward pass\n            # Input: captions without last token, Target: captions without first token\n            input_captions = captions[:, :-1]  # Remove <end> token\n            target_captions = captions[:, 1:]   # Remove <start> token\n            input_lengths = lengths - 1\n            \n            logits = self.model(images, input_captions, input_lengths)\n            \n            # Reshape for loss calculation\n            logits = logits.reshape(-1, logits.size(-1))  # [batch*seq, vocab]\n            targets = target_captions.reshape(-1)         # [batch*seq]\n            \n            loss = self.criterion(logits, targets)\n            \n            # Backward pass\n            self.optimizer.zero_grad()\n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n            \n            self.optimizer.step()\n            \n            total_loss += loss.item()\n        \n        return total_loss / num_batches\n    \n    def validate(self):\n        \"\"\"Validate the model\"\"\"\n        self.model.eval()\n        total_loss = 0\n        \n        with torch.no_grad():\n            for images, captions, lengths in tqdm.tqdm(self.val_loader):\n                images = images.to(self.device)\n                captions = captions.to(self.device)\n                lengths = lengths.to(self.device)\n                \n                # Same input/target split as training\n                input_captions = captions[:, :-1]\n                target_captions = captions[:, 1:]\n                input_lengths = lengths - 1\n                \n                logits = self.model(images, input_captions, input_lengths)\n                \n                logits = logits.reshape(-1, logits.size(-1))\n                targets = target_captions.reshape(-1)\n                \n                loss = self.criterion(logits, targets)\n                total_loss += loss.item()\n        \n        return total_loss / len(self.val_loader)\n    \n    def generate_sample_captions(self, num_samples=3):\n        \"\"\"Generate sample captions for monitoring progress - FIXED\"\"\"\n        self.model.eval()\n        \n        # Get a few validation samples\n        val_batch = next(iter(self.val_loader))\n        images, true_captions, _ = val_batch\n        \n        images = images[:num_samples].to(self.device)\n        \n        with torch.no_grad():\n            # Pass raw images, let the model handle encoding\n            generated = self.model(images)  # This will call generate_caption internally\n        \n        print(f\"\\nSample Generated Captions:\")\n        for i in range(num_samples):\n            # True caption\n            true_caption_tokens = [self.idx_to_word[idx.item()] for idx in true_captions[i] \n                                  if idx.item() != self.word_to_idx['<pad>']]\n            \n            # Generated caption\n            gen_caption_tokens = [self.idx_to_word[idx.item()] for idx in generated[i]]\n            \n            print(f\"True:      {' '.join(true_caption_tokens)}\")\n            print(f\"Generated: {' '.join(gen_caption_tokens)}\")\n            print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:12:38.873897Z","iopub.execute_input":"2025-08-13T21:12:38.874638Z","iopub.status.idle":"2025-08-13T21:12:38.892824Z","shell.execute_reply.started":"2025-08-13T21:12:38.874607Z","shell.execute_reply":"2025-08-13T21:12:38.891988Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import tqdm\n\ndef train_model(model, train_loader, val_loader, word_to_idx, idx_to_word,\n                num_epochs=10, device='cuda', resume_from_checkpoint=True):\n    \"\"\"Main training function\"\"\"\n    \n    # Move model to device\n    model = model.to(device)\n    \n    # Initialize training manager\n    trainer = TrainingManager(model, train_loader, val_loader, \n                            word_to_idx, idx_to_word, device=device)\n    \n    # Try to resume from checkpoint\n    if resume_from_checkpoint:\n        trainer.load_checkpoint('/kaggle/input/model/last_checkpoint.pth')\n    \n    print(f\"\\n🚀 Starting training for {num_epochs} epochs...\")\n    print(f\"Starting from epoch: {trainer.start_epoch}\")\n    \n    for epoch in range(trainer.start_epoch, num_epochs):\n        print(f\"\\n{'='*60}\")\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"{'='*60}\")\n        \n        # Training\n        print(\"Training...\")\n        train_loss = trainer.train_epoch()\n        trainer.train_losses.append(train_loss)\n        \n        # Validation  \n        print(\"Validating...\")\n        val_loss = trainer.validate()\n        trainer.val_losses.append(val_loss)\n        \n        # Learning rate scheduling\n        trainer.scheduler.step(val_loss)\n        current_lr = trainer.optimizer.param_groups[0]['lr']\n        \n        # Check if best model\n        is_best = val_loss < trainer.best_val_loss\n        if is_best:\n            trainer.best_val_loss = val_loss\n        \n        # Print epoch results\n        print(f\"\\nEpoch {epoch+1} Results:\")\n        print(f\"Train Loss: {train_loss:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f} {'🎉 (Best!)' if is_best else ''}\")\n        print(f\"Learning Rate: {current_lr:.6f}\")\n        \n        # Generate sample captions to monitor progress\n        trainer.generate_sample_captions(num_samples=2)\n        \n        # Save checkpoint\n        trainer.save_checkpoint(epoch, train_loss, val_loss, is_best)\n        \n        print(f\"Epoch {epoch+1} completed in {'-'*20}\")\n    \n    print(f\"\\n🎉 Training completed!\")\n    print(f\"Best validation loss: {trainer.best_val_loss:.4f}\")\n    \n    return trainer\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Start training\ntrainer = train_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    word_to_idx=word_to_idx,\n    idx_to_word=idx_to_word,\n    num_epochs=8,  # Start with 8 epochs\n    device=device,\n    resume_from_checkpoint=False  # Set True to resume\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T20:09:20.698667Z","iopub.execute_input":"2025-08-13T20:09:20.698921Z","iopub.status.idle":"2025-08-13T20:29:18.202800Z","shell.execute_reply.started":"2025-08-13T20:09:20.698905Z","shell.execute_reply":"2025-08-13T20:29:18.201839Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTraining setup complete!\nDevice: cuda\nTrainable parameters: 3,971,764\n\n🚀 Starting training for 8 epochs...\nStarting from epoch: 0\n\n============================================================\nEpoch 1/8\n============================================================\nTraining...\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 203/2023 [00:12<01:50, 16.53it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [200/2023], Loss: 3.2570\n","output_type":"stream"},{"name":"stderr","text":" 20%|█▉        | 402/2023 [00:23<01:35, 17.04it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [400/2023], Loss: 3.2423\n","output_type":"stream"},{"name":"stderr","text":" 30%|██▉       | 602/2023 [00:34<01:18, 18.15it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [600/2023], Loss: 3.2301\n","output_type":"stream"},{"name":"stderr","text":" 40%|███▉      | 801/2023 [00:46<01:13, 16.56it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [800/2023], Loss: 3.2105\n","output_type":"stream"},{"name":"stderr","text":" 50%|████▉     | 1003/2023 [00:58<00:58, 17.57it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1000/2023], Loss: 3.1921\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 1203/2023 [01:09<00:46, 17.68it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1200/2023], Loss: 3.1776\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 1402/2023 [01:21<00:35, 17.55it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1400/2023], Loss: 3.1635\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 1601/2023 [01:32<00:25, 16.65it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1600/2023], Loss: 3.1527\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▉ | 1803/2023 [01:44<00:12, 17.00it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1800/2023], Loss: 3.1469\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 2002/2023 [01:56<00:01, 17.16it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [2000/2023], Loss: 3.1394\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2023/2023 [01:57<00:00, 17.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validating...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 506/506 [00:29<00:00, 17.27it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Results:\nTrain Loss: 3.1374\nVal Loss: 3.0422 🎉 (Best!)\nLearning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\nSample Generated Captions:\nTrue:      <start> a blurred image of a black and white dog running through the grass <end>\nGenerated: <start> a brown dog is running through the grass <end>\n--------------------------------------------------\nTrue:      <start> a dog is running through a field <end>\nGenerated: <start> a brown dog is running through the grass <end>\n--------------------------------------------------\nSaved checkpoint: /kaggle/working/model_checkpoints/last_checkpoint.pth\n🎉 New best model saved: /kaggle/working/model_checkpoints/best_model.pth (val_loss: 3.0422)\nEpoch 1 completed in --------------------\n\n============================================================\nEpoch 2/8\n============================================================\nTraining...\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 203/2023 [00:12<01:43, 17.62it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [200/2023], Loss: 2.9554\n","output_type":"stream"},{"name":"stderr","text":" 20%|█▉        | 401/2023 [00:23<01:56, 13.93it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [400/2023], Loss: 2.9558\n","output_type":"stream"},{"name":"stderr","text":" 30%|██▉       | 601/2023 [00:35<01:19, 17.78it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [600/2023], Loss: 2.9644\n","output_type":"stream"},{"name":"stderr","text":" 40%|███▉      | 803/2023 [00:47<01:37, 12.49it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [800/2023], Loss: 2.9644\n","output_type":"stream"},{"name":"stderr","text":" 50%|████▉     | 1002/2023 [00:59<01:01, 16.63it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1000/2023], Loss: 2.9607\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 1201/2023 [01:11<00:47, 17.37it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1200/2023], Loss: 2.9611\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 1403/2023 [01:23<00:33, 18.45it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1400/2023], Loss: 2.9604\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 1603/2023 [01:34<00:23, 17.90it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1600/2023], Loss: 2.9576\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▉ | 1803/2023 [01:46<00:12, 17.59it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1800/2023], Loss: 2.9558\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 2003/2023 [01:57<00:01, 16.95it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [2000/2023], Loss: 2.9547\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2023/2023 [01:59<00:00, 16.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validating...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 506/506 [00:28<00:00, 17.76it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2 Results:\nTrain Loss: 2.9551\nVal Loss: 2.9986 🎉 (Best!)\nLearning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\nSample Generated Captions:\nTrue:      <start> a blurred image of a black and white dog running through the grass <end>\nGenerated: <start> a dog running through a field <end>\n--------------------------------------------------\nTrue:      <start> a dog is running through a field <end>\nGenerated: <start> a dog running through a field <end>\n--------------------------------------------------\nSaved checkpoint: /kaggle/working/model_checkpoints/last_checkpoint.pth\n🎉 New best model saved: /kaggle/working/model_checkpoints/best_model.pth (val_loss: 2.9986)\nEpoch 2 completed in --------------------\n\n============================================================\nEpoch 3/8\n============================================================\nTraining...\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 203/2023 [00:12<01:43, 17.63it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [200/2023], Loss: 2.8158\n","output_type":"stream"},{"name":"stderr","text":" 20%|█▉        | 403/2023 [00:24<01:29, 18.13it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [400/2023], Loss: 2.8328\n","output_type":"stream"},{"name":"stderr","text":" 30%|██▉       | 603/2023 [00:36<01:19, 17.87it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [600/2023], Loss: 2.8431\n","output_type":"stream"},{"name":"stderr","text":" 40%|███▉      | 802/2023 [00:48<01:10, 17.39it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [800/2023], Loss: 2.8493\n","output_type":"stream"},{"name":"stderr","text":" 50%|████▉     | 1002/2023 [00:59<00:58, 17.53it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1000/2023], Loss: 2.8525\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 1201/2023 [01:11<00:52, 15.62it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1200/2023], Loss: 2.8579\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 1401/2023 [01:23<00:39, 15.66it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1400/2023], Loss: 2.8618\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 1602/2023 [01:35<00:24, 17.29it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1600/2023], Loss: 2.8637\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▉ | 1801/2023 [01:47<00:13, 16.96it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1800/2023], Loss: 2.8672\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 2001/2023 [01:59<00:01, 15.06it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [2000/2023], Loss: 2.8681\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2023/2023 [02:00<00:00, 16.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validating...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 506/506 [00:33<00:00, 15.27it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3 Results:\nTrain Loss: 2.8685\nVal Loss: 2.9651 🎉 (Best!)\nLearning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\nSample Generated Captions:\nTrue:      <start> a blurred image of a black and white dog running through the grass <end>\nGenerated: <start> a brown dog is running through a grassy field <end>\n--------------------------------------------------\nTrue:      <start> a dog is running through a field <end>\nGenerated: <start> a brown dog is running through a grassy field <end>\n--------------------------------------------------\nSaved checkpoint: /kaggle/working/model_checkpoints/last_checkpoint.pth\n🎉 New best model saved: /kaggle/working/model_checkpoints/best_model.pth (val_loss: 2.9651)\nEpoch 3 completed in --------------------\n\n============================================================\nEpoch 4/8\n============================================================\nTraining...\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 203/2023 [00:14<01:58, 15.38it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [200/2023], Loss: 2.7709\n","output_type":"stream"},{"name":"stderr","text":" 20%|█▉        | 402/2023 [00:26<01:33, 17.25it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [400/2023], Loss: 2.7720\n","output_type":"stream"},{"name":"stderr","text":" 30%|██▉       | 602/2023 [00:39<01:31, 15.52it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [600/2023], Loss: 2.7800\n","output_type":"stream"},{"name":"stderr","text":" 40%|███▉      | 801/2023 [00:51<01:09, 17.60it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [800/2023], Loss: 2.7812\n","output_type":"stream"},{"name":"stderr","text":" 50%|████▉     | 1003/2023 [01:03<00:57, 17.80it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1000/2023], Loss: 2.7851\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 1202/2023 [01:15<00:46, 17.65it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1200/2023], Loss: 2.7938\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 1403/2023 [01:26<00:35, 17.28it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1400/2023], Loss: 2.7985\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 1600/2023 [01:41<00:33, 12.56it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1600/2023], Loss: 2.8055\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▉ | 1803/2023 [01:53<00:12, 17.96it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1800/2023], Loss: 2.8066\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 2002/2023 [02:05<00:01, 16.74it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [2000/2023], Loss: 2.8098\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2023/2023 [02:06<00:00, 15.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validating...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 506/506 [00:28<00:00, 17.74it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4 Results:\nTrain Loss: 2.8099\nVal Loss: 2.9119 🎉 (Best!)\nLearning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\nSample Generated Captions:\nTrue:      <start> a blurred image of a black and white dog running through the grass <end>\nGenerated: <start> a dog runs through the grass <end>\n--------------------------------------------------\nTrue:      <start> a dog is running through a field <end>\nGenerated: <start> a dog runs through the grass <end>\n--------------------------------------------------\nSaved checkpoint: /kaggle/working/model_checkpoints/last_checkpoint.pth\n🎉 New best model saved: /kaggle/working/model_checkpoints/best_model.pth (val_loss: 2.9119)\nEpoch 4 completed in --------------------\n\n============================================================\nEpoch 5/8\n============================================================\nTraining...\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 203/2023 [00:12<01:47, 16.99it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [200/2023], Loss: 2.6934\n","output_type":"stream"},{"name":"stderr","text":" 20%|█▉        | 403/2023 [00:24<01:34, 17.07it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [400/2023], Loss: 2.7232\n","output_type":"stream"},{"name":"stderr","text":" 30%|██▉       | 603/2023 [00:35<01:21, 17.51it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [600/2023], Loss: 2.7251\n","output_type":"stream"},{"name":"stderr","text":" 40%|███▉      | 802/2023 [00:47<01:08, 17.84it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [800/2023], Loss: 2.7443\n","output_type":"stream"},{"name":"stderr","text":" 50%|████▉     | 1003/2023 [00:58<00:52, 19.60it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1000/2023], Loss: 2.7473\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 1202/2023 [01:10<00:49, 16.66it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1200/2023], Loss: 2.7526\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 1403/2023 [01:22<00:32, 19.13it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1400/2023], Loss: 2.7528\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 1603/2023 [01:33<00:25, 16.46it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1600/2023], Loss: 2.7586\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▉ | 1803/2023 [01:44<00:12, 17.55it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1800/2023], Loss: 2.7643\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 2001/2023 [01:56<00:01, 17.28it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [2000/2023], Loss: 2.7704\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2023/2023 [01:57<00:00, 17.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validating...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 506/506 [00:29<00:00, 17.29it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5 Results:\nTrain Loss: 2.7706\nVal Loss: 2.9211 \nLearning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\nSample Generated Captions:\nTrue:      <start> a blurred image of a black and white dog running through the grass <end>\nGenerated: <start> a dog running across a field of flowers <end>\n--------------------------------------------------\nTrue:      <start> a dog is running through a field <end>\nGenerated: <start> a dog running across a field of flowers <end>\n--------------------------------------------------\nSaved checkpoint: /kaggle/working/model_checkpoints/last_checkpoint.pth\nEpoch 5 completed in --------------------\n\n============================================================\nEpoch 6/8\n============================================================\nTraining...\n","output_type":"stream"},{"name":"stderr","text":" 10%|▉         | 202/2023 [00:12<01:45, 17.23it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [200/2023], Loss: 2.6702\n","output_type":"stream"},{"name":"stderr","text":" 20%|█▉        | 403/2023 [00:24<01:30, 17.96it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [400/2023], Loss: 2.6884\n","output_type":"stream"},{"name":"stderr","text":" 30%|██▉       | 602/2023 [00:35<01:20, 17.73it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [600/2023], Loss: 2.6991\n","output_type":"stream"},{"name":"stderr","text":" 40%|███▉      | 803/2023 [00:47<01:06, 18.39it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [800/2023], Loss: 2.7090\n","output_type":"stream"},{"name":"stderr","text":" 50%|████▉     | 1002/2023 [00:59<01:00, 16.90it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1000/2023], Loss: 2.7108\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 1202/2023 [01:10<00:46, 17.59it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1200/2023], Loss: 2.7194\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 1402/2023 [01:22<00:37, 16.55it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1400/2023], Loss: 2.7202\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 1603/2023 [01:34<00:24, 17.02it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1600/2023], Loss: 2.7245\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▉ | 1802/2023 [01:45<00:12, 17.69it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1800/2023], Loss: 2.7308\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 2003/2023 [01:57<00:01, 15.67it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [2000/2023], Loss: 2.7340\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2023/2023 [01:58<00:00, 17.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validating...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 506/506 [00:28<00:00, 17.78it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6 Results:\nTrain Loss: 2.7352\nVal Loss: 2.8847 🎉 (Best!)\nLearning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\nSample Generated Captions:\nTrue:      <start> a blurred image of a black and white dog running through the grass <end>\nGenerated: <start> a dog running through the grass <end>\n--------------------------------------------------\nTrue:      <start> a dog is running through a field <end>\nGenerated: <start> a dog running through the grass <end>\n--------------------------------------------------\nSaved checkpoint: /kaggle/working/model_checkpoints/last_checkpoint.pth\n🎉 New best model saved: /kaggle/working/model_checkpoints/best_model.pth (val_loss: 2.8847)\nEpoch 6 completed in --------------------\n\n============================================================\nEpoch 7/8\n============================================================\nTraining...\n","output_type":"stream"},{"name":"stderr","text":" 10%|▉         | 202/2023 [00:12<01:44, 17.43it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [200/2023], Loss: 2.6385\n","output_type":"stream"},{"name":"stderr","text":" 20%|█▉        | 403/2023 [00:24<01:34, 17.17it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [400/2023], Loss: 2.6533\n","output_type":"stream"},{"name":"stderr","text":" 30%|██▉       | 603/2023 [00:35<01:23, 16.91it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [600/2023], Loss: 2.6691\n","output_type":"stream"},{"name":"stderr","text":" 40%|███▉      | 803/2023 [00:47<01:13, 16.66it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [800/2023], Loss: 2.6755\n","output_type":"stream"},{"name":"stderr","text":" 50%|████▉     | 1002/2023 [00:58<00:58, 17.48it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1000/2023], Loss: 2.6821\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 1203/2023 [01:10<00:46, 17.54it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1200/2023], Loss: 2.6875\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 1402/2023 [01:21<00:35, 17.31it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1400/2023], Loss: 2.6925\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 1603/2023 [01:33<00:26, 16.10it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1600/2023], Loss: 2.6997\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▉ | 1802/2023 [01:44<00:12, 17.73it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1800/2023], Loss: 2.7021\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 2002/2023 [01:56<00:01, 17.32it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [2000/2023], Loss: 2.7081\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2023/2023 [01:57<00:00, 17.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validating...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 506/506 [00:28<00:00, 17.70it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7 Results:\nTrain Loss: 2.7086\nVal Loss: 2.8808 🎉 (Best!)\nLearning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\nSample Generated Captions:\nTrue:      <start> a blurred image of a black and white dog running through the grass <end>\nGenerated: <start> a dog running through the grass <end>\n--------------------------------------------------\nTrue:      <start> a dog is running through a field <end>\nGenerated: <start> a dog running through the grass <end>\n--------------------------------------------------\nSaved checkpoint: /kaggle/working/model_checkpoints/last_checkpoint.pth\n🎉 New best model saved: /kaggle/working/model_checkpoints/best_model.pth (val_loss: 2.8808)\nEpoch 7 completed in --------------------\n\n============================================================\nEpoch 8/8\n============================================================\nTraining...\n","output_type":"stream"},{"name":"stderr","text":" 10%|▉         | 201/2023 [00:12<01:52, 16.25it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [200/2023], Loss: 2.6097\n","output_type":"stream"},{"name":"stderr","text":" 20%|█▉        | 403/2023 [00:23<01:30, 17.86it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [400/2023], Loss: 2.6292\n","output_type":"stream"},{"name":"stderr","text":" 30%|██▉       | 602/2023 [00:35<01:21, 17.38it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [600/2023], Loss: 2.6436\n","output_type":"stream"},{"name":"stderr","text":" 40%|███▉      | 802/2023 [00:47<01:21, 14.93it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [800/2023], Loss: 2.6514\n","output_type":"stream"},{"name":"stderr","text":" 50%|████▉     | 1002/2023 [00:58<01:00, 16.79it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1000/2023], Loss: 2.6632\n","output_type":"stream"},{"name":"stderr","text":" 59%|█████▉    | 1202/2023 [01:10<00:43, 18.67it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1200/2023], Loss: 2.6670\n","output_type":"stream"},{"name":"stderr","text":" 69%|██████▉   | 1403/2023 [01:22<00:35, 17.25it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1400/2023], Loss: 2.6711\n","output_type":"stream"},{"name":"stderr","text":" 79%|███████▉  | 1603/2023 [01:33<00:23, 17.73it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1600/2023], Loss: 2.6770\n","output_type":"stream"},{"name":"stderr","text":" 89%|████████▉ | 1803/2023 [01:44<00:12, 17.21it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [1800/2023], Loss: 2.6834\n","output_type":"stream"},{"name":"stderr","text":" 99%|█████████▉| 2001/2023 [01:56<00:01, 16.11it/s]","output_type":"stream"},{"name":"stdout","text":"Batch [2000/2023], Loss: 2.6842\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2023/2023 [01:58<00:00, 17.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validating...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 506/506 [00:29<00:00, 17.36it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8 Results:\nTrain Loss: 2.6850\nVal Loss: 2.8777 🎉 (Best!)\nLearning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\nSample Generated Captions:\nTrue:      <start> a blurred image of a black and white dog running through the grass <end>\nGenerated: <start> a dog running through the grass <end>\n--------------------------------------------------\nTrue:      <start> a dog is running through a field <end>\nGenerated: <start> a dog running through the grass <end>\n--------------------------------------------------\nSaved checkpoint: /kaggle/working/model_checkpoints/last_checkpoint.pth\n🎉 New best model saved: /kaggle/working/model_checkpoints/best_model.pth (val_loss: 2.8777)\nEpoch 8 completed in --------------------\n\n🎉 Training completed!\nBest validation loss: 2.8777\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_training_progress(trainer):\n    \"\"\"Plot training and validation losses\"\"\"\n    plt.figure(figsize=(12, 4))\n    \n    # Loss plot\n    plt.subplot(1, 2, 1)\n    epochs = range(1, len(trainer.train_losses) + 1)\n    plt.plot(epochs, trainer.train_losses, 'b-', label='Training Loss')\n    plt.plot(epochs, trainer.val_losses, 'r-', label='Validation Loss')\n    plt.title('Training Progress')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # Learning rate plot (if available)\n    plt.subplot(1, 2, 2)\n    # This will be useful to see LR scheduling\n    plt.title('Model Performance')\n    plt.text(0.5, 0.7, f'Best Val Loss: {trainer.best_val_loss:.4f}', \n             transform=plt.gca().transAxes, ha='center', fontsize=12)\n    plt.text(0.5, 0.5, f'Total Epochs: {len(trainer.train_losses)}', \n             transform=plt.gca().transAxes, ha='center', fontsize=12)\n    plt.text(0.5, 0.3, f'Final Train Loss: {trainer.train_losses[-1]:.4f}', \n             transform=plt.gca().transAxes, ha='center', fontsize=12)\n    plt.axis('off')\n    \n    plt.tight_layout()\n    plt.savefig('training_progress.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\n# After training completes, call this to visualize progress","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:12:52.189233Z","iopub.execute_input":"2025-08-13T21:12:52.189683Z","iopub.status.idle":"2025-08-13T21:12:52.196106Z","shell.execute_reply.started":"2025-08-13T21:12:52.189647Z","shell.execute_reply":"2025-08-13T21:12:52.195456Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"checkpoint.keys()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:29:18.619306Z","iopub.execute_input":"2025-08-13T21:29:18.619610Z","iopub.status.idle":"2025-08-13T21:29:18.626086Z","shell.execute_reply.started":"2025-08-13T21:29:18.619585Z","shell.execute_reply":"2025-08-13T21:29:18.625447Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'scheduler_state_dict', 'train_losses', 'val_losses', 'best_val_loss', 'train_loss', 'val_loss', 'word_to_idx', 'idx_to_word', 'timestamp'])"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"checkpoint['val_losses']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:29:21.184000Z","iopub.execute_input":"2025-08-13T21:29:21.184648Z","iopub.status.idle":"2025-08-13T21:29:21.190290Z","shell.execute_reply.started":"2025-08-13T21:29:21.184600Z","shell.execute_reply":"2025-08-13T21:29:21.189676Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[3.0421794080451545,\n 2.998595917413357,\n 2.9650969974137107,\n 2.911877240823663,\n 2.9211416072525056,\n 2.8846528888219902,\n 2.880760518929704,\n 2.8776617766369004]"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"!zip model_checkpoints /kaggle/working/model_checkpoints/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T20:30:16.947777Z","iopub.execute_input":"2025-08-13T20:30:16.948073Z","iopub.status.idle":"2025-08-13T20:30:23.269118Z","shell.execute_reply.started":"2025-08-13T20:30:16.948053Z","shell.execute_reply":"2025-08-13T20:30:23.268394Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/model_checkpoints/best_model.pth (deflated 10%)\n  adding: kaggle/working/model_checkpoints/last_checkpoint.pth (deflated 10%)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Create test split from remaining validation data\n# Since we did 80/20 split, let's take 50% of val for test\nval_images_list = list(val_images)\nval_test_images, val_val_images = train_test_split(\n    val_images_list, \n    test_size=0.5,  # Split validation in half\n    random_state=42\n)\n\nprint(f\"New validation images: {len(val_val_images)}\")\nprint(f\"Test images: {len(val_test_images)}\")\n\n# Create test caption dictionary\ntest_captions = {img: processed_captions[img] for img in val_test_images}\nupdated_val_captions = {img: processed_captions[img] for img in val_val_images}\n\n# Create test dataset\ntest_dataset = Flickr8kDataset(\n    image_dir='/kaggle/input/flickr8k/Images',\n    captions_dict=test_captions,\n    word_to_idx=word_to_idx,\n    transform=transform\n)\n\n# Create test loader\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=16,\n    shuffle=False,  # No shuffling for test set\n    collate_fn=collate_fn,\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"Test dataset size: {len(test_dataset)}\")\nprint(f\"Test batches: {len(test_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:29:26.681838Z","iopub.execute_input":"2025-08-13T21:29:26.682104Z","iopub.status.idle":"2025-08-13T21:29:26.696738Z","shell.execute_reply.started":"2025-08-13T21:29:26.682083Z","shell.execute_reply":"2025-08-13T21:29:26.695881Z"}},"outputs":[{"name":"stdout","text":"New validation images: 810\nTest images: 809\nDataset created with 4045 image-caption pairs\nTest dataset size: 4045\nTest batches: 253\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:30:08.773026Z","iopub.execute_input":"2025-08-13T21:30:08.773340Z","iopub.status.idle":"2025-08-13T21:30:08.778616Z","shell.execute_reply.started":"2025-08-13T21:30:08.773320Z","shell.execute_reply":"2025-08-13T21:30:08.777880Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"model = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:31:19.635011Z","iopub.execute_input":"2025-08-13T21:31:19.635678Z","iopub.status.idle":"2025-08-13T21:31:19.669086Z","shell.execute_reply.started":"2025-08-13T21:31:19.635651Z","shell.execute_reply":"2025-08-13T21:31:19.668458Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:30:28.355806Z","iopub.execute_input":"2025-08-13T21:30:28.356454Z","iopub.status.idle":"2025-08-13T21:30:28.359704Z","shell.execute_reply.started":"2025-08-13T21:30:28.356433Z","shell.execute_reply":"2025-08-13T21:30:28.359021Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport numpy as np\n\ndef evaluate_model(model, test_loader, idx_to_word, device):\n    \"\"\"Comprehensive model evaluation with BLEU scores\"\"\"\n    model.eval()\n    \n    all_bleu1_scores = []\n    all_bleu2_scores = []\n    all_bleu3_scores = []\n    all_bleu4_scores = []\n    \n    total_samples = 0\n    smoothing = SmoothingFunction().method1\n    \n    print(\"Evaluating model on test set...\")\n    print(\"=\" * 50)\n    \n    with torch.no_grad():\n        for batch_idx, (images, true_captions, lengths) in enumerate(tqdm.tqdm(test_loader)):\n            images = images.to(device)\n            batch_size = images.shape[0]\n            \n            # Generate captions\n            generated_captions = model(images)\n            \n            # Process each sample in batch\n            for i in range(batch_size):\n                # Convert true caption to words (remove padding)\n                true_tokens = []\n                for idx in true_captions[i]:\n                    if idx.item() == word_to_idx['<pad>']:\n                        break\n                    token = idx_to_word[idx.item()]\n                    if token not in ['<start>', '<end>']:  # Remove special tokens\n                        true_tokens.append(token)\n                \n                # Convert generated caption to words\n                gen_tokens = []\n                for idx in generated_captions[i]:\n                    token = idx_to_word[idx.item()]\n                    if token == '<end>':\n                        break\n                    if token not in ['<start>', '<pad>']:  # Remove special tokens\n                        gen_tokens.append(token)\n                \n                # Calculate BLEU scores\n                if len(gen_tokens) > 0 and len(true_tokens) > 0:\n                    # BLEU expects reference as list of lists\n                    reference = [true_tokens]\n                    candidate = gen_tokens\n                    \n                    # Calculate different BLEU scores\n                    bleu1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n                    bleu2 = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n                    bleu3 = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n                    bleu4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n                    \n                    all_bleu1_scores.append(bleu1)\n                    all_bleu2_scores.append(bleu2)\n                    all_bleu3_scores.append(bleu3)\n                    all_bleu4_scores.append(bleu4)\n                    \n                    total_samples += 1\n    \n    # Calculate average scores\n    avg_bleu1 = np.mean(all_bleu1_scores)\n    avg_bleu2 = np.mean(all_bleu2_scores)\n    avg_bleu3 = np.mean(all_bleu3_scores)\n    avg_bleu4 = np.mean(all_bleu4_scores)\n    \n    # Print results\n    print(f\"\\n🎯 EVALUATION RESULTS on {total_samples} samples:\")\n    print(\"=\" * 50)\n    print(f\"BLEU-1: {avg_bleu1:.4f}\")\n    print(f\"BLEU-2: {avg_bleu2:.4f}\") \n    print(f\"BLEU-3: {avg_bleu3:.4f}\")\n    print(f\"BLEU-4: {avg_bleu4:.4f} ⭐ (Primary Metric)\")\n    \n    # Interpretation\n    print(f\"\\n📊 INTERPRETATION:\")\n    if avg_bleu4 > 0.25:\n        print(\"🎉 Excellent performance! Research-level quality\")\n    elif avg_bleu4 > 0.15:\n        print(\"✅ Good performance! Acceptable for applications\")\n    elif avg_bleu4 > 0.10:\n        print(\"⚠️  Fair performance. Room for improvement\")\n    else:\n        print(\"❌ Poor performance. Needs significant improvement\")\n    \n    return {\n        'bleu1': avg_bleu1,\n        'bleu2': avg_bleu2, \n        'bleu3': avg_bleu3,\n        'bleu4': avg_bleu4,\n        'total_samples': total_samples\n    }\n\n# Run evaluation\nresults = evaluate_model(model, test_loader, idx_to_word, device)\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:31:22.186173Z","iopub.execute_input":"2025-08-13T21:31:22.186652Z","iopub.status.idle":"2025-08-13T21:31:43.245120Z","shell.execute_reply.started":"2025-08-13T21:31:22.186616Z","shell.execute_reply":"2025-08-13T21:31:43.244171Z"}},"outputs":[{"name":"stdout","text":"Evaluating model on test set...\n==================================================\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 253/253 [00:21<00:00, 12.03it/s]","output_type":"stream"},{"name":"stdout","text":"\n🎯 EVALUATION RESULTS on 4045 samples:\n==================================================\nBLEU-1: 0.2226\nBLEU-2: 0.1083\nBLEU-3: 0.0630\nBLEU-4: 0.0427 ⭐ (Primary Metric)\n\n📊 INTERPRETATION:\n❌ Poor performance. Needs significant improvement\n{'bleu1': 0.22264526954273994, 'bleu2': 0.10828822645810378, 'bleu3': 0.06303495889097521, 'bleu4': 0.04267422823700421, 'total_samples': 4045}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Test beam search\ndef test_beam_search(model, test_loader, idx_to_word, device, num_samples=5):\n    \"\"\"Test beam search vs greedy decoding\"\"\"\n    model.eval()\n    \n    images, true_captions, _ = next(iter(test_loader))\n    images = images[:num_samples].to(device)\n    \n    print(\"🔍 BEAM SEARCH vs GREEDY COMPARISON:\")\n    print(\"=\" * 70)\n    \n    with torch.no_grad():\n        # Greedy decoding\n        greedy_generated = model(images)\n        \n        # Beam search\n        beam_generated = model.generate_beam_search(images, beam_size=3)\n    \n    for i in range(num_samples):\n        # True caption\n        true_tokens = [idx_to_word[idx.item()] for idx in true_captions[i] \n                      if idx.item() != word_to_idx['<pad>']]\n        \n        # Greedy caption\n        greedy_tokens = [idx_to_word[idx.item()] for idx in greedy_generated[i]]\n        if '<end>' in greedy_tokens:\n            end_idx = greedy_tokens.index('<end>') + 1\n            greedy_tokens = greedy_tokens[:end_idx]\n        \n        # Beam search caption\n        beam_tokens = [idx_to_word[idx.item()] for idx in beam_generated[i]]\n        if '<end>' in beam_tokens:\n            end_idx = beam_tokens.index('<end>') + 1\n            beam_tokens = beam_tokens[:end_idx]\n        \n        print(f\"Sample {i+1}:\")\n        print(f\"True:   {' '.join(true_tokens)}\")\n        print(f\"Greedy: {' '.join(greedy_tokens)}\")\n        print(f\"Beam:   {' '.join(beam_tokens)}\")\n        print(\"-\" * 50)\n\n# Test it\ntest_beam_search(model, test_loader, idx_to_word, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:31:51.654767Z","iopub.execute_input":"2025-08-13T21:31:51.655078Z","iopub.status.idle":"2025-08-13T21:31:52.648785Z","shell.execute_reply.started":"2025-08-13T21:31:51.655054Z","shell.execute_reply":"2025-08-13T21:31:52.647914Z"}},"outputs":[{"name":"stdout","text":"🔍 BEAM SEARCH vs GREEDY COMPARISON:\n======================================================================\nSample 1:\nTrue:   <start> a boy smiles by the pool <end>\nGreedy: <start> a woman and a child are sitting on a bench <end>\nBeam:   <start> a man and a woman are sitting on a bench <end>\n--------------------------------------------------\nSample 2:\nTrue:   <start> a laughing young boy is near a swimming pool <end>\nGreedy: <start> a woman and a child are sitting on a bench <end>\nBeam:   <start> a man and a woman are sitting on a bench <end>\n--------------------------------------------------\nSample 3:\nTrue:   <start> a young boy outside by the pool smiling <end>\nGreedy: <start> a woman and a child are sitting on a bench <end>\nBeam:   <start> a man and a woman are sitting on a bench <end>\n--------------------------------------------------\nSample 4:\nTrue:   <start> the child is laughing by a pool on the other side of the bars <end>\nGreedy: <start> a woman and a child are sitting on a bench <end>\nBeam:   <start> a man and a woman are sitting on a bench <end>\n--------------------------------------------------\nSample 5:\nTrue:   <start> young boy in a dark green shirt behind a black fence near a pool <end>\nGreedy: <start> a woman and a child are sitting on a bench <end>\nBeam:   <start> a man and a woman are sitting on a bench <end>\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Better evaluation using all 5 captions per image\ndef evaluate_with_multiple_references(model, test_captions_dict, idx_to_word, device, num_samples=500):\n    \"\"\"Evaluate using all available captions for each image\"\"\"\n    model.eval()\n    \n    # Randomly sample some test images\n    test_images_list = list(test_captions_dict.keys())\n    sampled_images = np.random.choice(test_images_list, min(num_samples, len(test_images_list)), replace=False)\n    \n    all_bleu4_scores = []\n    \n    for img_name in sampled_images:\n        # Load and preprocess image\n        img_path = f'/kaggle/input/flickr8k/Images/{img_name}'\n        image = Image.open(img_path).convert('RGB')\n        image_tensor = transform(image).unsqueeze(0).to(device)\n        \n        # Generate caption\n        with torch.no_grad():\n            generated = model(image_tensor)\n        \n        # Convert generated to words\n        gen_tokens = []\n        for idx in generated[0]:\n            token = idx_to_word[idx.item()]\n            if token == '<end>':\n                break\n            if token not in ['<start>', '<pad>']:\n                gen_tokens.append(token)\n        \n        # Get all reference captions for this image\n        references = []\n        for caption in test_captions_dict[img_name]:\n            ref_tokens = []\n            for token in caption.split():\n                if token not in ['<start>', '<end>']:\n                    ref_tokens.append(token)\n            references.append(ref_tokens)\n        \n        # Calculate BLEU-4 with multiple references\n        if len(gen_tokens) > 0:\n            bleu4 = sentence_bleu(references, gen_tokens, weights=(0.25, 0.25, 0.25, 0.25), \n                                smoothing_function=SmoothingFunction().method1)\n            all_bleu4_scores.append(bleu4)\n    \n    avg_bleu4 = np.mean(all_bleu4_scores)\n    print(f\"BLEU-4 with multiple references: {avg_bleu4:.4f}\")\n    return avg_bleu4\n\n# Test with multiple references\nmulti_ref_bleu4 = evaluate_with_multiple_references(model, test_captions, idx_to_word, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T21:32:42.255168Z","iopub.execute_input":"2025-08-13T21:32:42.255532Z","iopub.status.idle":"2025-08-13T21:33:02.013416Z","shell.execute_reply.started":"2025-08-13T21:32:42.255492Z","shell.execute_reply":"2025-08-13T21:33:02.012756Z"}},"outputs":[{"name":"stdout","text":"BLEU-4 with multiple references: 0.1275\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}